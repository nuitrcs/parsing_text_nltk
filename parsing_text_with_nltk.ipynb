{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnATUhr7VYBf"
      },
      "source": [
        "# Parsing text with NLTK üìö\n",
        "\n",
        "This notebook introduces [NLTK](https://www.nltk.org/), which is one of Python's key libraries for natural language processing (NLP).\n",
        "\n",
        "NLP is broadly about using computers to manipulate the type of text that humans use for everyday communication. This can be very simple or very complicated. This notebook focuses on the simpler end of the spectrum, on which some other methods build.\n",
        "\n",
        "NLTK was created in 2001 at the University of Pennsylvania and is widely embraced in academia. There are other libraries, such as [spaCy](https://spacy.io/), which are also helpful and commonly used. One advantage of NLTK is that it provides lots of customization.\n",
        "\n",
        "This notebook focuses mainly on how to parse text to create bags of words. In other words, this notebook teaches how to go from raw text (a string) to a list of tokens (e.g., words) that can be used for different purposes.\n",
        "\n",
        "Let's get started! üí™üèº\n",
        "\n",
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebeY3kjdVdaI",
        "outputId": "116a34b9-633a-43de-bb6c-c7d3b70ff66b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # To work with data frames\n",
        "import re # To work with regular expressions\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvW7e-s0WpeW"
      },
      "source": [
        "## Reading data\n",
        "\n",
        "This notebook will be working with posts from Reddit's API. The posts are the \"hotest\" posts in four subreddits: Meditation, Mindfulness, Headspace, and Buddhism. (You can see [how the posts where collected in this URL](https://github.com/emiliolehoucq/mindfulness/blob/main/data_collection_reddit_4_17_2024.ipynb).)\n",
        "\n",
        "The data are hosted on GitHub. We can use the URL to the raw data directly in `pd.read_csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTKb6_G3WqN2"
      },
      "outputs": [],
      "source": [
        "df_reddit = pd.read_csv('https://raw.githubusercontent.com/emiliolehoucq/mindfulness/main/data_raw_reddit_4_17_2024.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHdqV7uoXB0X"
      },
      "source": [
        "## Taking a quick look at the data üëÄ\n",
        "\n",
        "Before going any further into NLTK and NLP, let's get to know the data that we will be working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE2qPk3GXRN8",
        "outputId": "bb814355-2d91-40d1-f0a1-3142b6519c1b"
      },
      "outputs": [],
      "source": [
        "df_reddit.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S-d1OQLXyZx"
      },
      "source": [
        "3513 rows and 14 columns..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuG8MTONXUYa",
        "outputId": "e6bd5864-e96c-4666-f2f6-0646987fcf27"
      },
      "outputs": [],
      "source": [
        "df_reddit.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbaSmtYlX2jX"
      },
      "source": [
        "You can find what each column means in [PRAW's documentation](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjl3mDYbXXXz",
        "outputId": "0e3b7024-47d4-40f6-c2a8-2b4e5e0a2c54"
      },
      "outputs": [],
      "source": [
        "df_reddit.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrORbu2wYYpo"
      },
      "source": [
        "Aha, we seem to have some text! Let's see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "_ECiZCPZW2kr",
        "outputId": "34f1e482-47a4-4aea-9469-60318dfca0f6"
      },
      "outputs": [],
      "source": [
        "df_reddit.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L68xHD8nW6yP"
      },
      "source": [
        "We'll be working mostly with `selftext`, the text of the post, and `title`, the title of the post. Here's one example of `selftext`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "giMgPOV3Ynb5",
        "outputId": "d9e99691-98a1-4f98-c9d3-6290aa4733c9"
      },
      "outputs": [],
      "source": [
        "IDX = 194\n",
        "post = df_reddit['selftext'][IDX]\n",
        "post"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPxlR4IsY3NY"
      },
      "source": [
        "And one example of `title`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LECMOcpUYwP1",
        "outputId": "66ee7737-2728-4763-9265-3e0d0003b2a0"
      },
      "outputs": [],
      "source": [
        "title = df_reddit['title'][IDX]\n",
        "title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvF2MDz8StYJ"
      },
      "source": [
        "`selftext` has some missing values (try `df_reddit['selftext'].isnull().mean()`), so we're dropping them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27a15cetScCN"
      },
      "outputs": [],
      "source": [
        "df_reddit.dropna(subset=['selftext'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMY3uoDbZFsH"
      },
      "source": [
        "Okay, ready to parse text with NLTK!\n",
        "\n",
        "## Processing raw text for computing\n",
        "\n",
        "We have a fair amount of raw text (strings). üò± We'll call each of the posts and titles a *document*. How can we compute with those documents? We need a numerical representation.\n",
        "\n",
        "In this notebook, we'll focus on the \"*bag of words*\" model, basically turning documents into lists of words and counting how many times each word appears in the document.\n",
        "\n",
        "How can we turn documents into lists of words? *Tokenization*! Tokenization consists of splitting a string into linguistic units. When we tokenize a string, we produce a list of *tokens*, or sequences of characters that we want to treat as a group. For now, we're going to focus on words.\n",
        "\n",
        "For example, `\"This is a sentence\"` would turn into `[\"This\", \"is\", \"a\", \"sentence\"]`. Easy, no? ü§î Let's try splitting on white spaces:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j4eoZ-xaOv-",
        "outputId": "cb075e8c-3a47-4cfc-a148-eaa094f2d7fb"
      },
      "outputs": [],
      "source": [
        "post.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNPgkxn6hnOD"
      },
      "source": [
        "That seems to work pretty well. However, there are a couple of problems, two of which are:\n",
        "- Some tokens include punctuation or parentheses\n",
        "- 'I‚Äôve', 'consistency-sake.', etc. are actually two words\n",
        "\n",
        "We could improve our approach using regular expressions (or ways to describe patterns in text). Python provides a character class for word characters (`\\w`), which is equivalent to `[a-zA-Z0-9_]`. There's also a complement to this class (`\\W`). We could use that complement to split strings into anything other than word characters (i.e., anything other than letters, digits, and underscores):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISXtM-b_kEI-",
        "outputId": "1246d1e3-c2f1-4357-db35-99f56f239a64"
      },
      "outputs": [],
      "source": [
        "re.split(r'\\W+', post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH150quelxTw"
      },
      "source": [
        "That works pretty well in this case! However, we get empty strings at the end. Of course, we could solve this problem and continue improving our regular expression to account for various cases. However, if you've ever worked with text and regular expressions, you know that gets tricky very quickly... üòµ‚Äçüí´\n",
        "\n",
        "Fortunately, NLTK provides a tokenizer that you can use ([several tokenizers](https://www.nltk.org/api/nltk.tokenize.html), actually): üòÖ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVF7Iwf3m8D3",
        "outputId": "5185d95e-8dd9-4cb3-9fe2-12ecddaad218"
      },
      "outputs": [],
      "source": [
        "nltk.word_tokenize(post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZqq0hK-yrI_"
      },
      "source": [
        "As you can see, it's still not perfect (e.g., we get 'consistency-sake' again!). üò¨\n",
        "\n",
        "Still, `word_tokenize` is a more sophisticated tokenizer than most of us could probably write. It's NLTK's recommented word tokenizer, which builds on [`TreebankWordTokenizer`](https://www.nltk.org/api/nltk.tokenize.TreebankWordTokenizer.html#nltk.tokenize.TreebankWordTokenizer) (that uses regular expressions) and [`PunktSentenceTokenizer`](https://www.nltk.org/api/nltk.tokenize.PunktSentenceTokenizer.html#nltk.tokenize.PunktSentenceTokenizer) (a sentence tokenizer that uses an unsupervised learning algorithm to build a model and then uses the model to find sentence boundaries). (It's because `word_tokenize` uses the Punkt sentence tokenization models that we needed to download it above: `nltk.download('punkt')`.)\n",
        "\n",
        "Tokenization turns out to be pretty difficult. üò• Also, there's no solution that fits all cases. What counts as a token could depend on your application. To check the quality of your tokenization, it's useful to have some text that has been manually tokenized to compare the output of the tokenizer with what you want. For this notebook, let's move on...\n",
        "\n",
        "## Exercise 1\n",
        "\n",
        "Select a different post than the one we've been working with, apply the different methods for tokenizing covered above, and compare the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8uti6hC7Rwo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ztc8cB1m7DlM"
      },
      "source": [
        "Wasn't that fun?! ü§©\n",
        "\n",
        "Now, let's tokenize every post and store the result in a new column in the data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5El7b_PfiAd"
      },
      "outputs": [],
      "source": [
        "df_reddit['selftext_tokenized'] = df_reddit['selftext'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F0SPKXX4J1P"
      },
      "source": [
        "- `df_reddit['selftext_tokenized'] = ` creates a new column\n",
        "- `df_reddit['selftext'].apply(nltk.word_tokenize)` applies `nltk.word_tokenize` to every cell of `df_reddit['selftext']`\n",
        "\n",
        "Did we get what we expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZOi5teye4AGg",
        "outputId": "9acaa4d3-a432-4aa4-df8f-0cdff1597850"
      },
      "outputs": [],
      "source": [
        "df_reddit[['selftext_tokenized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhFuMdAm5uR1"
      },
      "source": [
        "Seems about right! Let's take a look at the post that we've been working with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRZGsyst4FUM",
        "outputId": "7c47d28d-4927-4a14-87ab-dd4fe1287334"
      },
      "outputs": [],
      "source": [
        "df_reddit['selftext_tokenized'][IDX]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "barbF4c24c8P"
      },
      "source": [
        "## Exercise 2 ü´µüèº\n",
        "\n",
        "Tokenize every title (`title`) and store the result in a new column in the data frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYgAebTc4djd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUVmQjkF5zAv"
      },
      "source": [
        "Great! Now we have a list of words. What sort of cool insights can we get from that?\n",
        "\n",
        "## Frequency distributions\n",
        "\n",
        "A frequency distribution is a collection of items along with their frequency counts. A frequency distribution can help identify the words that are most informative of a document or a collection of documents.\n",
        "\n",
        "Do users talk about different things in the subreddits about meditation v mindfulness v Headspace v Buddhism? Let's find out!\n",
        "\n",
        "First, let's combine the list of words for each of the subreddits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0DgSGuJCLEe"
      },
      "outputs": [],
      "source": [
        "meditation = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_tokenized'].sum()\n",
        "mindfulness = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_tokenized'].sum()\n",
        "headspace = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_tokenized'].sum()\n",
        "buddhism = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_tokenized'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEfO8YYsCnG_"
      },
      "source": [
        "As you can see, we get big lists of words (exciting!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP055ysrCrjm",
        "outputId": "c4ba1f94-2b2c-454f-f08e-91753ecd5e05"
      },
      "outputs": [],
      "source": [
        "print(f'Type of meditation: {type(meditation)}')\n",
        "print(f'Length of meditation: {len(meditation)}')\n",
        "print()\n",
        "print(f'Type of mindfulness: {type(mindfulness)}')\n",
        "print(f'Length of mindfulness: {len(mindfulness)}')\n",
        "print()\n",
        "print(f'Type of headspace: {type(headspace)}')\n",
        "print(f'Length of headspace: {len(headspace)}')\n",
        "print()\n",
        "print(f'Type of buddhism: {type(buddhism)}')\n",
        "print(f'Length of buddhism: {len(buddhism)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuuFP3yOC8bt"
      },
      "source": [
        "Now, let's create our frequency distributions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q01lLCaDC_jz"
      },
      "outputs": [],
      "source": [
        "freq_dist_meditation = nltk.FreqDist(meditation)\n",
        "freq_dist_mindfulness = nltk.FreqDist(mindfulness)\n",
        "freq_dist_headspace = nltk.FreqDist(headspace)\n",
        "freq_dist_buddhism = nltk.FreqDist(buddhism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NURJE0p1DC7G"
      },
      "source": [
        "As you can see, we use the function [`FreqDist`](https://www.nltk.org/api/nltk.probability.FreqDist.html). Now, let's see the most common words for each of the four subreddits... ü´£"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mxIcvcWC_nG",
        "outputId": "aba11fd2-1baa-4550-fef9-41333f6260b7"
      },
      "outputs": [],
      "source": [
        "print(f'Most common words in meditation: {freq_dist_meditation.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in mindfulness: {freq_dist_mindfulness.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in headspace: {freq_dist_headspace.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in buddhism: {freq_dist_buddhism.most_common(10)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcVLGcEZCFgz"
      },
      "source": [
        "Wow, that was UNDERWHELMING! üò≠ The most common words are not informative at all! Sometimes they're not even words. Okay, let's fix that...\n",
        "\n",
        "Before, though, your turn to create frequency distributions:\n",
        "\n",
        "## Exercise 3\n",
        "\n",
        "Create frequency distributions for the titles for each subreddit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15_wkAB3RPxa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkcvgqo2RQMU"
      },
      "source": [
        "## Tokenization 2.0: removing stopwords, removing punctuation, and lowercasing ü§ûüèΩ\n",
        "\n",
        "Our approach to tokenization so far has several problems, particularly:\n",
        "- The most common words are not very informative. They are often so-called *stopwords*, or high-frequency words that we often filter out before further processing since they're not very distinctive of a document or collection of documents. You may also want to add common words for your application (e.g., \"meditation\" in the case of this notebook).\n",
        "- We're including punctuation in the bag of words, which is often not useful. We don't care if the posts in the Meditation subreddit use \".\" more or less often than the posts in the \"Mindfulness\" subreddit.\n",
        "- The words are capitalized differently. So, we're counting \"Mindfulness\" and \"mindfulness\" as two different words.\n",
        "\n",
        "Fortunately, these problems are not too hard to solve. Let's do it! We'll start by creating a function that we can apply to each row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3Ty8JX33nMi"
      },
      "outputs": [],
      "source": [
        "# Add custom words to the list of stop words\n",
        "# Remember code above:\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = nltk.corpus.stopwords.words('english')\n",
        "stop_words = stop_words + [\n",
        "    # You can optionally add words here\n",
        "]\n",
        "\n",
        "# In this case, we also want to remove URLs from the text before parsing it\n",
        "# Taken from: https://www.geeksforgeeks.org/remove-urls-from-string-in-python/\n",
        "def remove_urls(text, replacement_text=\"\"):\n",
        "    # Define a regex pattern to match URLs\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+') # https?:// protocol (optional s), \\S+ one or more non-white space characters, | or, www\\.\\S+ URLs starting with www.\n",
        "    # Use the sub() method to replace URLs with the specified replacement text\n",
        "    text_without_urls = url_pattern.sub(replacement_text, text)\n",
        "    return text_without_urls\n",
        "\n",
        "# Define custom function for tokenization\n",
        "def my_tokenizer(post):\n",
        "  \"\"\"\n",
        "  Function to tokenize a post.\n",
        "  Input: post.\n",
        "  Output: list of tokens.\n",
        "  Dependencies: NLTK\n",
        "  \"\"\"\n",
        "  # Remove URLs from post\n",
        "  post = remove_urls(post)\n",
        "  # Convert post to string and tokenize it\n",
        "  tokens = nltk.word_tokenize(str(post))\n",
        "  # Clean tokens:\n",
        "  # - Check that it's alphanumeric (a-z and 0-9)\n",
        "  # - Remove stopwords\n",
        "  # - Lowercase\n",
        "  tokens = [token.lower() for token in tokens if token.isalnum() and token.lower() not in stop_words]\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoEHDJ3FHzxJ"
      },
      "source": [
        "Let's try our function with the post we've already seen a number of times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9JzljSr4Fjo",
        "outputId": "8ee86db8-8740-4e01-e87d-09b354326a10"
      },
      "outputs": [],
      "source": [
        "my_tokenizer(post)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrrXjMT7H4JR"
      },
      "source": [
        "Seems to be working well! Let's tokenize posts and create the frequency distributions again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSvIw5dpILr2"
      },
      "outputs": [],
      "source": [
        "# Tokenize posts using are custom function\n",
        "df_reddit['selftext_tokenized'] = df_reddit['selftext'].apply(my_tokenizer)\n",
        "\n",
        "# Create a list of words for each subreddit\n",
        "meditation = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_tokenized'].sum()\n",
        "mindfulness = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_tokenized'].sum()\n",
        "headspace = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_tokenized'].sum()\n",
        "buddhism = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_tokenized'].sum()\n",
        "\n",
        "# Create a frequency distribution for each subreddit\n",
        "freq_dist_meditation = nltk.FreqDist(meditation)\n",
        "freq_dist_mindfulness = nltk.FreqDist(mindfulness)\n",
        "freq_dist_headspace = nltk.FreqDist(headspace)\n",
        "freq_dist_buddhism = nltk.FreqDist(buddhism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUYkmhZbKxCZ"
      },
      "source": [
        "In this case, let's visualize the results instead of printing a list of tuples: üìâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GD-HsXgAIObG",
        "outputId": "7a121996-7ccf-47c9-95d0-60cc88759a4a"
      },
      "outputs": [],
      "source": [
        "NUM_RESULTS = 25\n",
        "freq_dist_meditation.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness.plot(NUM_RESULTS)\n",
        "freq_dist_headspace.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism.plot(NUM_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zq2w1JuMSn-"
      },
      "source": [
        "We get some interesting results, but nothing all that exciting. Let's try a couple of other methods!\n",
        "\n",
        "## Tokenization 3.0: stemming, lemmatizing, and n-grams ü§ûüèΩü§ûüèΩ\n",
        "\n",
        "Our approach to tokenization so far has two problems that we can solve:\n",
        "- We're treating differently words that convey similar information (e.g., \"meditation\" and \"meditate\" or \"mindfulness\" and \"mindful\"). Fortunately, we can address this problem with stemming and lemmatization. Stemming consists in keeping only the stem of a word, removing \"the end\" of the word (e.g., \"manufactur\" instead of \"manufacturing\"). Lemmatizing is similar to stemming, but making sure that the resulting word exists in the dictionary. Since lemmatizing involves further computation, it is slower than stemming. Fortunately, NLTK includes off-the-shelf stemmers and lemmatizers! However, as it is more generally the case with tokenizing, you should select the stemmer/lemmatizer that best fits your needs.\n",
        "- We're focusing on single words (or *unigrams*). However, sometimes the meaning of words changes when they're together with another/other word(s) (e.g., \"White House\" vs \"White\" and \"House\" or \"Thich Nhat Hanh\" vs \"Thich\", \"Nhat\", and \"Hanh\"). To address this problem, we can tokenize posts into *n-grams*, particularly *bi-grams* and *tri-grams*, or ordered sets of n, 2, and 3 words, respectively. This increases the number of unique tokens, but retains more information.\n",
        "\n",
        "Let's try it out! Let's improve the function we had created for tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi2wVBYJRoc5"
      },
      "outputs": [],
      "source": [
        "# Define new custom function for tokenization\n",
        "def my_tokenizer_improved(post):\n",
        "  \"\"\"\n",
        "  Improved function to tokenize a post.\n",
        "  Input: post.\n",
        "  Output: tuple with lists of tokens.\n",
        "  Dependencies: NLTK\n",
        "  \"\"\"\n",
        "  # Remove URLs from post\n",
        "  post = remove_urls(post)\n",
        "  # Convert post to string and tokenize it\n",
        "  tokens = nltk.word_tokenize(str(post))\n",
        "  # Create variables to store unigrams, stems, and lemmas\n",
        "  unigrams = []\n",
        "  stems = []\n",
        "  lemmas = []\n",
        "  # Clean tokens, stem, and lemmatize\n",
        "  for token in tokens:\n",
        "    # Lowercase\n",
        "    token = token.lower()\n",
        "    # Keep only alphanumeric and remove stopwords\n",
        "    if token.isalnum() and token not in stop_words:\n",
        "      # Store unigram\n",
        "      unigrams.append(token)\n",
        "      # Stemming\n",
        "      stems.append(nltk.PorterStemmer().stem(token))\n",
        "      # Lemmatizing\n",
        "      lemmas.append(nltk.WordNetLemmatizer().lemmatize(token)) # nltk.download('wordnet') is necessary for this\n",
        "  # Create lists of bigrams\n",
        "  bigrams = list(nltk.bigrams(unigrams))\n",
        "  bigrams_stems = list(nltk.bigrams(stems))\n",
        "  bigrams_lemmas = list(nltk.bigrams(lemmas))\n",
        "  # Create lists of trigrams\n",
        "  trigrams = list(nltk.trigrams(unigrams))\n",
        "  trigrams_stems = list(nltk.trigrams(stems))\n",
        "  trigrams_lemmas = list(nltk.trigrams(lemmas))\n",
        "  # Return tuple with everything\n",
        "  return unigrams, stems, lemmas, bigrams, bigrams_stems, bigrams_lemmas, trigrams, trigrams_stems, trigrams_lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L0HRGkg0UHY"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "By now you're somewhat familiar with the workflow that we're using to tokenize text, create frequency distributions, and visualize them. Shortly we'll apply `my_tokenizer_improved` to the posts. Before, though, try doing it yourself for `title`. You can start by comparing what you get with unigrams, stems, and lemmas. No worries if you can't do it. Trying it may help you identify questions that we can answer throughout the rest of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIkFH08n0U-5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81LPpyu8fbh8"
      },
      "source": [
        "Let's apply `my_tokenizer_improved` to the posts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKWGjpu1bX5Y"
      },
      "outputs": [],
      "source": [
        "# Columns to store results\n",
        "df_reddit['selftext_unigrams'] = None\n",
        "df_reddit['selftext_stems'] = None\n",
        "df_reddit['selftext_lemmas'] = None\n",
        "df_reddit['selftext_bigrams'] = None\n",
        "df_reddit['selftext_bigrams_stems'] = None\n",
        "df_reddit['selftext_bigrams_lemmas'] = None\n",
        "df_reddit['selftext_trigrams'] = None\n",
        "df_reddit['selftext_trigrams_stems'] = None\n",
        "df_reddit['selftext_trigrams_lemmas'] = None\n",
        "\n",
        "# Iterating over rows of the data frame storing results\n",
        "for i, row in df_reddit.iterrows():\n",
        "  # Tokenize post\n",
        "  unigrams, stems, lemmas, bigrams, bigrams_stems, bigrams_lemmas, trigrams, trigrams_stems, trigrams_lemmas = my_tokenizer_improved(str(row['selftext']))\n",
        "  # Store results\n",
        "  df_reddit.at[i, 'selftext_unigrams'] = unigrams\n",
        "  df_reddit.at[i, 'selftext_stems'] = stems\n",
        "  df_reddit.at[i, 'selftext_lemmas'] = lemmas\n",
        "  df_reddit.at[i, 'selftext_bigrams'] = bigrams\n",
        "  df_reddit.at[i, 'selftext_bigrams_stems'] = bigrams_stems\n",
        "  df_reddit.at[i, 'selftext_bigrams_lemmas'] = bigrams_lemmas\n",
        "  df_reddit.at[i, 'selftext_trigrams'] = trigrams\n",
        "  df_reddit.at[i, 'selftext_trigrams_stems'] = trigrams_stems\n",
        "  df_reddit.at[i, 'selftext_trigrams_lemmas'] = trigrams_lemmas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvppPDzLbX-r"
      },
      "source": [
        "Let's quickly see what we get:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        },
        "id": "59S80RAKbYBM",
        "outputId": "c934937a-11c1-4756-c092-5ecf25bb40e7"
      },
      "outputs": [],
      "source": [
        "df_reddit.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jqJjXgKp0Hc"
      },
      "source": [
        "Great, let's now create the lists for each subreddit and the frequency distributions. Let's start with stems:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8uT27df8bYDq",
        "outputId": "61e38f7c-94b4-4440-e3d4-66105a1bc7cf"
      },
      "outputs": [],
      "source": [
        "# Create lists for each subreddit\n",
        "meditation_stems = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_stems'].sum()\n",
        "mindfulness_stems = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_stems'].sum()\n",
        "headspace_stems = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_stems'].sum()\n",
        "buddhism_stems = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_stems'].sum()\n",
        "\n",
        "# Create frequency distributions for each subreddit\n",
        "freq_dist_meditation_stems = nltk.FreqDist(meditation_stems)\n",
        "freq_dist_mindfulness_stems = nltk.FreqDist(mindfulness_stems)\n",
        "freq_dist_headspace_stems = nltk.FreqDist(headspace_stems)\n",
        "freq_dist_buddhism_stems = nltk.FreqDist(buddhism_stems)\n",
        "\n",
        "# Visualizing results\n",
        "freq_dist_meditation_stems.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness_stems.plot(NUM_RESULTS)\n",
        "freq_dist_headspace_stems.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism_stems.plot(NUM_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrO2RTRqqq-7"
      },
      "source": [
        "Not particularly useful in this case üòî. Let's do lemmas now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tNP4YEH2qyLe",
        "outputId": "b8f5cedc-4aa9-4e4a-9a34-c74e42158921"
      },
      "outputs": [],
      "source": [
        "# Create lists for each subreddit\n",
        "meditation_lemmas = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_lemmas'].sum()\n",
        "mindfulness_lemmas = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_lemmas'].sum()\n",
        "headspace_lemmas = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_lemmas'].sum()\n",
        "buddhism_lemmas = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_lemmas'].sum()\n",
        "\n",
        "# Create frequency distributions for each subreddit\n",
        "freq_dist_meditation_lemmas = nltk.FreqDist(meditation_lemmas)\n",
        "freq_dist_mindfulness_lemmas = nltk.FreqDist(mindfulness_lemmas)\n",
        "freq_dist_headspace_lemmas = nltk.FreqDist(headspace_lemmas)\n",
        "freq_dist_buddhism_lemmas = nltk.FreqDist(buddhism_lemmas)\n",
        "\n",
        "# Visualizing results\n",
        "freq_dist_meditation_lemmas.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness_lemmas.plot(NUM_RESULTS)\n",
        "freq_dist_headspace_lemmas.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism_lemmas.plot(NUM_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZFqGu6rIs-"
      },
      "source": [
        "Again, not particularly informative üò´. Let's do bigrams now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vqQgzoDkrTU7",
        "outputId": "dd4c46cd-dfaf-4b6f-d234-62f7f148e008"
      },
      "outputs": [],
      "source": [
        "# Create lists for each subreddit\n",
        "meditation_bigrams = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_bigrams'].sum()\n",
        "mindfulness_bigrams = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_bigrams'].sum()\n",
        "headspace_bigrams = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_bigrams'].sum()\n",
        "buddhism_bigrams = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_bigrams'].sum()\n",
        "\n",
        "# Create frequency distributions for each subreddit\n",
        "freq_dist_meditation_bigrams = nltk.FreqDist(meditation_bigrams)\n",
        "freq_dist_mindfulness_bigrams = nltk.FreqDist(mindfulness_bigrams)\n",
        "freq_dist_headspace_bigrams = nltk.FreqDist(headspace_bigrams)\n",
        "freq_dist_buddhism_bigrams = nltk.FreqDist(buddhism_bigrams)\n",
        "\n",
        "# Visualizing results\n",
        "freq_dist_meditation_bigrams.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness_bigrams.plot(NUM_RESULTS)\n",
        "freq_dist_headspace_bigrams.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism_bigrams.plot(NUM_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPZs9YlCrtAN"
      },
      "source": [
        "Now we're getting more juice out of it! üßÉ Please notice how the frequency is much lower, though.\n",
        "\n",
        "Let's go straight to trigrams for time sake:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZQ173e1Ir5ss",
        "outputId": "3ee3e8b0-4327-4d6d-f61e-45a0b636fc7b"
      },
      "outputs": [],
      "source": [
        "# Create lists for each subreddit\n",
        "meditation_trigrams = df_reddit[df_reddit['subreddit'] == 'Meditation']['selftext_trigrams'].sum()\n",
        "mindfulness_trigrams = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['selftext_trigrams'].sum()\n",
        "headspace_trigrams = df_reddit[df_reddit['subreddit'] == 'Headspace']['selftext_trigrams'].sum()\n",
        "buddhism_trigrams = df_reddit[df_reddit['subreddit'] == 'Buddhism']['selftext_trigrams'].sum()\n",
        "\n",
        "# Create frequency distributions for each subreddit\n",
        "freq_dist_meditation_trigrams = nltk.FreqDist(meditation_trigrams)\n",
        "freq_dist_mindfulness_trigrams = nltk.FreqDist(mindfulness_trigrams)\n",
        "freq_dist_headspace_trigrams = nltk.FreqDist(headspace_trigrams)\n",
        "freq_dist_buddhism_trigrams = nltk.FreqDist(buddhism_trigrams)\n",
        "\n",
        "# Visualizing results\n",
        "freq_dist_meditation_trigrams.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness_trigrams.plot(NUM_RESULTS)\n",
        "freq_dist_headspace_trigrams.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism_trigrams.plot(NUM_RESULTS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTEHA4MtsgZW"
      },
      "source": [
        "This one is also pretty interesting! ü•≥\n",
        "\n",
        "Again, please note that the frequency gets lower.\n",
        "\n",
        "This is as far as we'll get in this notebook... but please note that we could keep trying other approaches to make frequency distributions more informative. For example, we could look only at the long words, at least those that occur relatively frequently (which could be more characteristic and informative), or focusing on bigrams that occur more often than we would expect based on the frequency of the individual words ([collocations, which you can find using NLTK](https://www.nltk.org/api/nltk.collocations.html?highlight=collocation#module-nltk.collocations)).\n",
        "\n",
        "## Bonus: Exercise 5\n",
        "\n",
        "Haven't had enough yet? Try creating frequency distributions for only the long words in either `selftext` or `title` (or both!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2gApQaf1vJ1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttfya5fJOnKa"
      },
      "source": [
        "## Conclusion üé¨\n",
        "\n",
        "This notebook provided an introduction to parsing text with NLTK. We discussed the bag of words model, tokenization, and frequency distributions focusing on how to implement them with NLTK. We covered various approaches to tokenization, including aspects such as removing stopwords and punctuation, lowercasing, stemming and lemmatizing, and using bigrams and tri-grams.\n",
        "\n",
        "Now that you know how to parse text with NLTK, the sky is the limit! Besides calculating frequency distributions, you can use lists of tokens with other methods such as topic modeling (Latent Dirichlet Allocation) and (lexicon-based) sentiment analysis.\n",
        "\n",
        "## Resources to continue learning üîñ\n",
        "\n",
        "This workshop draws from other materials that you can consult as well to continue learning:\n",
        "\n",
        "- Tutorial: [Natural Lanugage Processing with Python's NLTK Package](https://realpython.com/nltk-nlp-python/)\n",
        "- Book (available online): [Natural Language Processing with Python](https://search.library.northwestern.edu/permalink/01NWU_INST/h04e76/alma9962172594202441)\n",
        "- Book (available in the library): [Text as Data](https://search.library.northwestern.edu/permalink/01NWU_INST/h04e76/alma9982095900202441)\n",
        "\n",
        "## Answers to the exercises\n",
        "\n",
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYBzEIr66F6b",
        "outputId": "b4a72d8f-1791-45cd-f621-e17062d09a26"
      },
      "outputs": [],
      "source": [
        "post_exercise = df_reddit['selftext'][900]\n",
        "\n",
        "print(post_exercise.split())\n",
        "print()\n",
        "print(re.split(r'\\W+', post_exercise))\n",
        "print()\n",
        "print(nltk.word_tokenize(post_exercise))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAUiVe-c_dkp"
      },
      "source": [
        "### Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "vu_-8f_B_fdv",
        "outputId": "7c797e2f-0fd5-4efa-e705-36428ef546d5"
      },
      "outputs": [],
      "source": [
        "df_reddit['title_tokenized'] = df_reddit['title'].apply(nltk.word_tokenize)\n",
        "df_reddit[['title_tokenized']].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLd0vKyQ_gCK"
      },
      "source": [
        "### Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byLWqkO8_h0i",
        "outputId": "d9812eff-cd28-4182-91b3-6463ed838801"
      },
      "outputs": [],
      "source": [
        "# Create lists of words for each subreddit\n",
        "meditation_title = df_reddit[df_reddit['subreddit'] == 'Meditation']['title_tokenized'].sum()\n",
        "mindfulness_title = df_reddit[df_reddit['subreddit'] == 'Mindfulness']['title_tokenized'].sum()\n",
        "headspace_title = df_reddit[df_reddit['subreddit'] == 'Headspace']['title_tokenized'].sum()\n",
        "buddhism_title = df_reddit[df_reddit['subreddit'] == 'Buddhism']['title_tokenized'].sum()\n",
        "\n",
        "# Create frequency distributions for each subreddit\n",
        "freq_dist_meditation_title = nltk.FreqDist(meditation_title)\n",
        "freq_dist_mindfulness_title = nltk.FreqDist(mindfulness_title)\n",
        "freq_dist_headspace_title = nltk.FreqDist(headspace_title)\n",
        "freq_dist_buddhism_title = nltk.FreqDist(buddhism_title)\n",
        "\n",
        "# Print most common words\n",
        "print(f'Most common words in meditation: {freq_dist_meditation_title.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in mindfulness: {freq_dist_mindfulness_title.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in headspace: {freq_dist_headspace_title.most_common(10)}')\n",
        "print()\n",
        "print(f'Most common words in buddhism: {freq_dist_buddhism_title.most_common(10)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6DbswIU_iQn"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "The answer was provided above.\n",
        "\n",
        "### Exercise 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MHV1fmKi_jzr",
        "outputId": "a4377b67-30a8-48b1-9236-32794a50d9c2"
      },
      "outputs": [],
      "source": [
        "# Let's focus on the unigrams we created\n",
        "\n",
        "# Length threshold\n",
        "LEN = 10\n",
        "\n",
        "# Get only long words\n",
        "meditation_long = [token for token in meditation if len(token) > LEN]\n",
        "mindfulness_long = [token for token in mindfulness if len(token) > LEN]\n",
        "headspace_long = [token for token in headspace if len(token) > LEN]\n",
        "buddhism_long = [token for token in buddhism if len(token) > LEN]\n",
        "\n",
        "# Create a frequency distribution for each subreddit\n",
        "freq_dist_meditation_long = nltk.FreqDist(meditation_long)\n",
        "freq_dist_mindfulness_long = nltk.FreqDist(mindfulness_long)\n",
        "freq_dist_headspace_long = nltk.FreqDist(headspace_long)\n",
        "freq_dist_buddhism_long = nltk.FreqDist(buddhism_long)\n",
        "\n",
        "# Visualizing results\n",
        "freq_dist_meditation_long.plot(NUM_RESULTS)\n",
        "freq_dist_mindfulness_long.plot(NUM_RESULTS)\n",
        "freq_dist_headspace_long.plot(NUM_RESULTS)\n",
        "freq_dist_buddhism_long.plot(NUM_RESULTS)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
